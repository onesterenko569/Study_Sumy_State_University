import nltk # імпортуємо основну бібліотеку з nltk
from nltk.corpus import gutenberg, stopwords # імпортуємо дві частини модуля для завантаження тексту та для роботи з стоп-словами
from nltk.probability import FreqDist # для підрахунку частоти кожного унікального слова в тексті
import matplotlib.pyplot as plt # для побудови стовпчастих діаграм

# визначимо кількість слів у тексті
words = gutenberg.words('bryant-stories.txt') # імпортуємо текст із електронного архіву текстів Project Gutenberg і токенізуємо його (розділяємо по словах)
print("Кількість слів у тексті:", len(words)) # визначення та виведення кількості слів

# визначимо 10 найбільш вживаних слів у тексті та побудуємо на основі цих даних стовпчасту діаграму
fdist = FreqDist(words) # створюємо словник, де ключ - унікальний токен, значення - кількість токена в тексті
top_ten = fdist.most_common(10) # отримуємо список з 10 найбільш вживаних слів у тексті (слово, к-сть)
print("\n10 найбільш вживаних слів:")
for w, c in top_ten: # цикл по елементам списку top_ten
    print(f"\nСлово: {w}\nКількість цього слова в тексті: {c}") # виведення 10 найбльш вживаніших слів

# побудова стовпчастої діаграми
labels = [w for w, c in top_ten] # створюємо список назв стовпців
counts = [c for w, c in top_ten] # створюємо список кількості повторень слова в тексті

plt.figure(figsize=(10, 5)) # задаємо розмір графіка
plt.bar(labels, counts) # будуємо стовпчасту діаграму
plt.title("Топ-10 найбільш вживаних слів (до очищення)") # встановлюємо заголовок діаграми
plt.xlabel("Слова") # підписуємо вісь х
plt.ylabel("Частота") # підписуємо вісь у
plt.show() # показує створений графік

# видалення стоп-слів та пунктуації
stop_words = stopwords.words("english") # отримуємо та зберігаємо стоп-слова

# переписуємо текст без стоп-слів та знаків пунктуації
without_stop_words  = \
[
    w.lower() for w in words # цикл по словах
    if w.isalpha() # перевірка чи слово складається з букв і не є пунктуаційним символом
    and w.lower() not in stop_words # перевірка чи є слово стоп-словом
]

# повторне знаходження в тексті 10 найбільш вживаних слів
fdist_clean = FreqDist(without_stop_words) # створюємо словник, де ключ - унікальний токен, значення - кількість токена в тексті
top_ten_clean = fdist_clean.most_common(10) # отримуємо список з 10 найбільш вживаних слів у тексті (слово, к-сть)
print("\n10 найбільш вживаних слів після очищення:")
for w, c in top_ten_clean: # цикл по елементам списку top_ten_clean
    print(f"\nСлово: {w}\nКількість цього слова в тексті: {c}") # виведення 10 найбльш вживаніших слів

# побудова другої стовпчастої діаграми
labels_clean = [w for w, c in top_ten_clean] # створюємо список назв стовпців
counts_clean = [c for w, c in top_ten_clean] # створюємо список кількості повторень слова в тексті

plt.figure(figsize=(10, 5)) # задаємо розмір графіка
plt.bar(labels_clean, counts_clean) # будуємо стовпчасту діаграму
plt.title("Топ-10 найбільш вживаних слів (після очищення)") # встановлюємо заголовок діаграми
plt.xlabel("Слова") # підписуємо вісь х
plt.ylabel("Частота") # підписуємо вісь у
plt.show() # показує створений графік
